{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
      ],
      "metadata": {
        "id": "6WYMfvCNPwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from typing import Union\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "builder = tfds.builder('mnist')\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 12\n",
        "NUM_CLASSES = 10  # 10 total classes."
      ],
      "metadata": {
        "id": "vA8ppgB2P0aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "H2EFLQROP2R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test splits.\n",
        "def preprocess(x):\n",
        "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
        "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
        "  return image, subclass_labels\n",
        "\n",
        "\n",
        "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
        "mnist_train = mnist_train.map(preprocess)\n",
        "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
        "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "mnist_test = tfds.load('mnist', split='test').cache()\n",
        "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "ynByMG_UP4A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model creation"
      ],
      "metadata": {
        "id": "kAZwfvW5P63q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "# Build CNN teacher.\n",
        "cnn_model = tf.keras.Sequential()\n",
        "\n",
        "# your code start from here for stpe 2\n",
        "\n",
        "\n",
        "\n",
        "# Build fully connected student.\n",
        "fc_model = tf.keras.Sequential()\n",
        "\n",
        "\n",
        "# your code start from here for step 2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zINgDkA7P7BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher loss function"
      ],
      "metadata": {
        "id": "8JWGucyrQGav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_teacher_loss(images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  subclass_logits = cnn_model(images, training=True)\n",
        "\n",
        "  # Compute cross-entropy loss for subclasses.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "  cross_entropy_loss_value =\n",
        "\n",
        "\n",
        "  return cross_entropy_loss_value"
      ],
      "metadata": {
        "id": "DhzBP6ZLQJ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student loss function"
      ],
      "metadata": {
        "id": "JS8xkuH0QbOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "# Hyperparameters for distillation (need to be tuned).\n",
        "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  \"\"\"Compute distillation loss.\n",
        "\n",
        "  This function computes cross entropy between softened logits and softened\n",
        "  targets. The resulting loss is scaled by the squared temperature so that\n",
        "  the gradient magnitude remains approximately constant as the temperature is\n",
        "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
        "  a neural network.\"\n",
        "\n",
        "  Args:\n",
        "    teacher_logits: A Tensor of logits provided by the teacher.\n",
        "    student_logits: A Tensor of logits provided by the student, of the same\n",
        "      shape as `teacher_logits`.\n",
        "    temperature: Temperature to use for distillation.\n",
        "\n",
        "  Returns:\n",
        "    A scalar Tensor containing the distillation loss.\n",
        "  \"\"\"\n",
        " # your code start from here for step 3\n",
        "  soft_targets = \n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  student_subclass_logits = fc_model(images, training=True)\n",
        "\n",
        "  # Compute subclass distillation loss between student subclass logits and\n",
        "  # softened teacher subclass targets probabilities.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "\n",
        "  teacher_subclass_logits = cnn_model(images, training=False)\n",
        "  distillation_loss_value =\n",
        "\n",
        "  # Compute cross-entropy loss with hard targets.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "\n",
        "  cross_entropy_loss_value = \n",
        "\n",
        "  return "
      ],
      "metadata": {
        "id": "lDKia4gPQMIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluation"
      ],
      "metadata": {
        "id": "RJ1uyvurQ3w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_num_correct(model, images, labels):\n",
        "  \"\"\"Compute number of correctly classified images in a batch.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Number of correctly classified images.\n",
        "  \"\"\"\n",
        "  class_logits = model(images, training=False)\n",
        "  return tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "  \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "  \"\"\"\n",
        "\n",
        "  # your code start from here for step 4\n",
        "  optimizer = \n",
        "\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Run training.\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images, labels in mnist_train:\n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "\n",
        "        loss_value = \n",
        "\n",
        "      grads = \n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = builder.info.splits['test'].num_examples\n",
        "    for images, labels in mnist_test:\n",
        "      # your code start from here for step 4\n",
        "      num_correct += \n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n"
      ],
      "metadata": {
        "id": "EtoLbp8uQ4Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training models"
      ],
      "metadata": {
        "id": "NQL1lJdaRPT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 5 \n",
        "\n"
      ],
      "metadata": {
        "id": "-AGHbyABRPz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test accuracy vs. tempreture curve"
      ],
      "metadata": {
        "id": "sj1N38fnRTNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 6\n"
      ],
      "metadata": {
        "id": "gX4dbazrRWIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train student from scratch"
      ],
      "metadata": {
        "id": "WNrH_1emRbGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build fully connected student.\n",
        "fc_model_no_distillation = tf.keras.Sequential()\n",
        "\n",
        "# your code start from here for step 7\n",
        "\n",
        "\n",
        "\n",
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "def compute_plain_cross_entropy_loss(images, labels):\n",
        "  \"\"\"Compute plain loss for given images and labels.\n",
        "\n",
        "  For fair comparison and convenience, this function also performs a\n",
        "  LogSumExp over subclasses, but does not perform subclass distillation.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  # your code start from here for step 7\n",
        "\n",
        "  student_subclass_logits = fc_model_no_distillation(images, training=True)\n",
        "  cross_entropy_loss = \n",
        "  \n",
        "  return cross_entropy_loss\n",
        "\n",
        "\n",
        "train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss)"
      ],
      "metadata": {
        "id": "HjospsxIRbQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the teacher and student model (number of of parameters and FLOPs) "
      ],
      "metadata": {
        "id": "yq3JTpQ4RuhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 8\n"
      ],
      "metadata": {
        "id": "4V8GB2yRRuxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the state-of-the-art KD algorithm"
      ],
      "metadata": {
        "id": "KjwJ5oziRvRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 12\n"
      ],
      "metadata": {
        "id": "q10lybAFRvZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) XAI method to explain models"
      ],
      "metadata": {
        "id": "6dsOmtqdieIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 13\n"
      ],
      "metadata": {
        "id": "X0IMIFW8ilPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}